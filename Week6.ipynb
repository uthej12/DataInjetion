{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Used \n",
    "https://www.kaggle.com/new-york-city/nyc-parking-tickets?select=Parking_Violations_Issued_-_Fiscal_Year_2015.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile file.yaml\n",
    "file_type: csv\n",
    "dataset_name: Parking_Violations_2015\n",
    "file_name: Parking_Violations_2015\n",
    "inbound_delimiter: \",\"\n",
    "outbound_delimiter: \"|\"\n",
    "skip_leading_rows: 1\n",
    "columns: \n",
    "    - Summons Number \n",
    "    - Plate ID \n",
    "    - Registration State\n",
    "    - Plate Type\n",
    "    - Issue Date \n",
    "    - Violation Code \n",
    "    - Vehicle Body Type\n",
    "    - Vehicle Make\n",
    "    - Issuing Agency \n",
    "    - Street Code1\n",
    "    - Street Code2\n",
    "    - Street Code3 \n",
    "    - Vehicle Expiration Date\n",
    "    - Violation Location\n",
    "    - Violation Precinct\n",
    "    - Issuer Precinct\n",
    "    - Issuer Code\n",
    "    - Issuer Command \n",
    "    - Issuer Squad\n",
    "    - Violation Time\n",
    "    - Time First Observed\n",
    "    - Violation County \n",
    "    - Violation In Front Of Or Opposite\n",
    "    - House Number\n",
    "    - Street Name\n",
    "    - Intersecting Street\n",
    "    - Date First Observed\n",
    "    - Law Section \n",
    "    - Sub Division \n",
    "    - Violation Legal Code\n",
    "    - Days Parking In Effect\n",
    "    - From Hours In Effect\n",
    "    - To Hours In Effect\n",
    "    - Vehicle Color\n",
    "    - Unregistered Vehicle\n",
    "    - Vehicle Year\n",
    "    - Meter Number\n",
    "    - Feet From Curb\n",
    "    - Violation Post Code\n",
    "    - Violation Description\n",
    "    - No Standing or Stopping Violation\n",
    "    - Hydrant Violation\n",
    "    - Double Parking Violation\n",
    "    - Latitude\n",
    "    - Longitude \n",
    "    - Community Board\n",
    "    - Community Council \n",
    "    - Census Tract\n",
    "    - BIN\n",
    "    - BBL\n",
    "    - NTA\n",
    "drop_columns:\n",
    "    - No Standing or Stopping Violation\n",
    "    - Hydrant Violation\n",
    "    - Double Parking Violation\n",
    "    - Latitude\n",
    "    - Longitude\n",
    "    - Community Board\n",
    "    - Community Council \n",
    "    - Census Tract\n",
    "    - BIN\n",
    "    - BBL\n",
    "    - NTA\n",
    "    - Violation Legal Code\n",
    "    - Time First Observed\n",
    "    - Unregistered Vehicle\n",
    "    - Meter Number\n",
    "    - Violation County\n",
    "    - Violation In Front Of Or Opposite\n",
    "    - House Number\n",
    "    - Intersecting Street\n",
    "    - Days Parking In Effect\n",
    "    - From Hours In Effect\n",
    "    - To Hours In Effect\n",
    "    - Meter Number\n",
    "    - Violation Post Code\n",
    "    \n",
    "dtypes:\n",
    "    - Feet From Curb: float64\n",
    "    - Law Section: float64\n",
    "    - Vehicle Year: float64\n",
    "    - Violation Legal Code: object\n",
    "    - Double Parking Violation: object\n",
    "    - Hydrant Violation: object\n",
    "    - No Standing or Stopping Violation: object\n",
    "drop_null: True\n",
    "output_file_name: final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile testutility.py\n",
    "import pandas as pd\n",
    "import yaml\n",
    "    \n",
    "def array_clean(array):\n",
    "    new_array = [(''.join(y for y in x if y.isalnum() or y==' ')).strip().lower() for x in array]\n",
    "    return new_array\n",
    "\n",
    "def config_file(path):\n",
    "    with open(path,'r') as file:\n",
    "        att = yaml.safe_load(file)\n",
    "    return att\n",
    "\n",
    "def column_validation(df,expected):\n",
    "    new_column_names = array_clean(df.columns)\n",
    "    expected_column_names = array_clean(expected)\n",
    "    trigger = True\n",
    "    unexpected_columns = [x for x in new_column_names if x not in expected_column_names]\n",
    "    if len(unexpected_columns) >0:\n",
    "        print('Columns Not Present in Schema',unexpected_columns)\n",
    "        trigger = False\n",
    "    missing_columns = [x for x in expected_column_names if x not in new_column_names]\n",
    "    if len(missing_columns) >0:\n",
    "        print('Missing Columns',missing_columns)\n",
    "        trigger = False\n",
    "    if trigger:\n",
    "        df.columns = new_column_names\n",
    "        print('Sucessfully validated column Names')\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def drop_columns(df,col_names):\n",
    "    df = df.drop(array_clean(col_names),axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import testutility as utils\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "att = utils.config_file(\"file.yaml\")\n",
    "\n",
    "file_name = att['file_name']\n",
    "extension = att['file_type']\n",
    "\n",
    "print('Reading file .....')\n",
    "r_start = time.time()\n",
    "df = pd.read_csv(file_name+'.'+extension, delimiter = att['inbound_delimiter'])\n",
    "r_end = time.time()\n",
    "print('File Read sucessfully, time taken:',(r_end-e_start))\n",
    "\n",
    "\n",
    "print('Validating columns.....')\n",
    "if utils.column_validation(df,att['columns']):\n",
    "    print(\"Validation Sucessfull\")\n",
    "else:\n",
    "    print('Validation failed , process stopped')\n",
    "\n",
    "if len(att['drop_columnns']) >0:\n",
    "    print('Deleting specified columns....')\n",
    "    df = utils.drop_columns(df,att['drop_columns'])\n",
    "\n",
    "if att['drop_null']:\n",
    "    print('Dropping NULL Rows....')\n",
    "    df = df.dropna()\n",
    "    \n",
    "\n",
    "c_start = time.time()\n",
    "print('Compressing the dataset.....')\n",
    "df.to_csv(att['output_file_name']+'.csv.gz', sep='|', compression='gzip')\n",
    "c_end = time.time()\n",
    "print('Sucessfully compressed dataset, time taken:',(c_end-c_start))\n",
    "\n",
    "print('Writing Summary File....')\n",
    "no_of_rows = len(df)\n",
    "no_of_cols = len(df.columns)\n",
    "file_size = os.path.getsize(att['output_file_name']+'.csv.gz')\n",
    "\n",
    "with open('summary.txt', 'w') as f:\n",
    "    f.write('Number of rows:'+ str(no_of_rows))\n",
    "    f.write('\\n')\n",
    "    f.write('Number of columns:'+ str(no_of_cols))\n",
    "    f.write('\\n')\n",
    "    f.write('File Size:'+ str(file_size))\n",
    "print('Sucessfully written summary file')\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import testutility as util\n",
    "import time \n",
    "\n",
    "start = time.time()\n",
    "att = utils.config_file(\"file.yaml\")\n",
    "\n",
    "file_name = att['file_name']\n",
    "extension = att['file_type']\n",
    "dt = {list(x.keys())[0]:list(x.values())[0] for x in att['dtypes']}\n",
    "\n",
    "print('Reading file .....')\n",
    "r_start = time.time()\n",
    "df = dd.read_csv(file_name+'.'+ extension, dtype = dt)\n",
    "r_end = time.time()\n",
    "print('File Read sucessfully, time taken:',(r_end-e_start))\n",
    "\n",
    "print('Validating columns.....')\n",
    "if utils.column_validation(df,att['columns']):\n",
    "    print(\"Validation Sucessfull\")\n",
    "else:\n",
    "    print('Validation failed , process stopped')\n",
    "    \n",
    "\n",
    "if len(att['drop_columnns']) >0:\n",
    "    print('Deleting specified columns....')\n",
    "    df = utils.drop_columns(df,att['drop_columns'])\n",
    "\n",
    "df = df.compute()\n",
    "\n",
    "if att['drop_null']:\n",
    "    print('Dropping NULL Rows....')\n",
    "    df = df.dropna()\n",
    "\n",
    "    \n",
    "c_start = time.time()\n",
    "print('Compressing the dataset.....')\n",
    "df.to_csv(att['output_file_name']+'.csv.gz', sep='|', compression='gzip')\n",
    "c_end = time.time()\n",
    "print('Sucessfully compressed dataset, time taken:',(c_end-c_start))\n",
    "\n",
    "no_of_rows = len(df)\n",
    "no_of_cols = len(df.columns)\n",
    "file_size = os.path.getsize(att['output_file_name']+'.csv.gz')\n",
    "\n",
    "print('Writing Summary File....')\n",
    "no_of_rows = len(df)\n",
    "no_of_cols = len(df.columns)\n",
    "file_size = os.path.getsize(att['output_file_name']+'.csv.gz')\n",
    "\n",
    "with open('summary.txt', 'w') as f:\n",
    "    f.write('Number of rows:'+ str(no_of_rows))\n",
    "    f.write('\\n')\n",
    "    f.write('Number of columns:'+ str(no_of_cols))\n",
    "    f.write('\\n')\n",
    "    f.write('File Size:'+ str(file_size))\n",
    "print('Sucessfully written summary file')\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
